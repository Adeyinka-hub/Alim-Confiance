{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# **Important Note**\n",
    "**READ ME :**\n",
    "\n",
    "The final solution has been splitted in several notebooks.\n",
    "- 1. Create features\n",
    "- 2. Train LGBM (8 CPU) + inference on test data\n",
    "- 3. Train XGBOOST and CATBOOST (GPU) + inference on test data\n",
    "- 4. Final ensemble and submission (this notebook)\n",
    "    \n",
    "I will provide all notebooks if needed at the end of the competition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# **Librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# **Open Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/trustii-processed-data/\"\n",
    "data_path = \"Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df       = pd.read_csv('/kaggle/input/trustii-aim/train.csv')\n",
    "#test_df  = pd.read_csv('/kaggle/input/trustii-aim/test.csv')\n",
    "\n",
    "\n",
    "df       = pd.read_csv(data_path + 'train.csv')\n",
    "test_df  = pd.read_csv(data_path + 'test.csv')\n",
    "\n",
    "\n",
    "print(test_df.shape)\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dico_y = {'A corriger de manière urgente' : 0,\n",
    "          'A améliorer' : 1,\n",
    "          'Satisfaisant' : 2,\n",
    "          'Très satisfaisant' : 3,\n",
    "         }\n",
    "dico_y_INVERSE = {v:k for k, v in dico_y.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = ['A corriger de manière urgente',\n",
    "                  'A améliorer',\n",
    "                  'Satisfaisant',\n",
    "                  'Très satisfaisant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# **Combine oof**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,\n",
    "              pd.read_csv('OOF/train_oof_lgbm.csv'),\n",
    "              how='left',\n",
    "              on='Numero_inspection',\n",
    "             )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              pd.read_csv('OOF/train_oof_xgb_catboost.csv'),\n",
    "              how='left',\n",
    "              on='Numero_inspection',\n",
    "             )\n",
    "\n",
    "print(df.shape)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.merge(test_df,\n",
    "                   pd.read_csv('OOF/test_oof_lgbm.csv').drop(columns = ['trustii_id']),\n",
    "                   how='left',\n",
    "                   on='Numero_inspection',\n",
    "                  )\n",
    "\n",
    "test_df = pd.merge(test_df,\n",
    "                   pd.read_csv('OOF/test_oof_xgb_catboost.csv').drop(columns = ['trustii_id']),\n",
    "                   how='left',\n",
    "                   on='Numero_inspection',\n",
    "                  )\n",
    "\n",
    "print(test_df.shape)\n",
    "display(test_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# **Ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_oof_ = [x for x in df if x.startswith('oof_') and not(x.startswith('oof_class')) and df[x].dtypes != 'str' and '_param' in x]\n",
    "\n",
    "cols_oof = []\n",
    "for x in cols_oof_ :\n",
    "    for i_ in target_classes :\n",
    "        i = '_' + i_\n",
    "        if not(x.endswith(i)) :\n",
    "            continue\n",
    "        new_x = x.replace(i, '')\n",
    "        if new_x not in cols_oof :\n",
    "            cols_oof.append(new_x)\n",
    "            \n",
    "cols_pred = [x.replace('oof_', 'pred_') for x in cols_oof]\n",
    "print(cols_oof)\n",
    "print(cols_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = {'w0': 0.080504,\n",
    "           'w1': 0.054101,\n",
    "           'w2': 0.070493,\n",
    "           'w3': 0.097491,\n",
    "           'w4': 0.001788,\n",
    "           'w5': 0.082893,\n",
    "           'w6': 0.039908,\n",
    "           'w7': 0.072237,\n",
    "           'w8': 0.073922,\n",
    "           'w9': 0.005372,\n",
    "           'w10': 0.100776,\n",
    "           'w11': 0.001794,\n",
    "           'w12': 0.003368,\n",
    "           'w13': 0.0531,\n",
    "           'w14': 0.044548,\n",
    "           'w15': 0.045245,\n",
    "           'w16': 0.019479,\n",
    "           'w17': 0.085234,\n",
    "           'w18': 0.067748}\n",
    "\n",
    "# Uniform\n",
    "S = np.sum(list(weights.values()))\n",
    "weights = {k:v/S for k, v in weights.items()}\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create ensemble-preds\n",
    "for t_class in target_classes :\n",
    "    test_df[f'ensemble_pred_{t_class}'] = np.average(test_df[[f\"{col}_{t_class}\" for col in cols_pred]].values, weights=list(weights.values()), axis=1)\n",
    "        \n",
    "test_df['Synthese_eval_sanit'] = test_df[[f'ensemble_pred_{i}' for i in target_classes]].idxmax(axis=1).apply(lambda x : x.replace('ensemble_pred_', ''))\n",
    "\n",
    "test_df['Synthese_eval_sanit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_class in target_classes :\n",
    "    df[f'ensemble_pred_{t_class}'] = np.average(df[[f\"{col}_{t_class}\" for col in cols_oof]].values, weights=list(weights.values()), axis=1)\n",
    "        \n",
    "predictions = df[[f'ensemble_pred_{i}' for i in target_classes]].idxmax(axis=1).apply(lambda x : x.replace('ensemble_pred_', ''))\n",
    "    \n",
    "print(\"CV :\", accuracy_score(df['Synthese_eval_sanit'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = test_df[['trustii_id', 'Synthese_eval_sanit']].reset_index(drop=True)\n",
    "sub['trustii_id'] = sub['trustii_id'].astype(int)\n",
    "\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(sub.shape)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['Synthese_eval_sanit'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4478141,
     "sourceId": 7686139,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4486177,
     "sourceId": 7691370,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DACC_standard - Python",
   "language": "python",
   "name": "conda-env-DACC_standard-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

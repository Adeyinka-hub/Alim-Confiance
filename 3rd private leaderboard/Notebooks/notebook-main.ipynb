{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./trustii/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in ./trustii/lib/python3.10/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy in ./trustii/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: xgboost in ./trustii/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: optuna in ./trustii/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: gplearn in ./trustii/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: mlxtend in ./trustii/lib/python3.10/site-packages (0.23.1)\n",
      "Requirement already satisfied: seaborn in ./trustii/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in ./trustii/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: hyperopt in ./trustii/lib/python3.10/site-packages (0.2.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./trustii/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./trustii/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./trustii/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./trustii/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./trustii/lib/python3.10/site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./trustii/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: tqdm in ./trustii/lib/python3.10/site-packages (from optuna) (4.66.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./trustii/lib/python3.10/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./trustii/lib/python3.10/site-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: colorlog in ./trustii/lib/python3.10/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: PyYAML in ./trustii/lib/python3.10/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in ./trustii/lib/python3.10/site-packages (from optuna) (2.0.27)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./trustii/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pillow>=8 in ./trustii/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./trustii/lib/python3.10/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./trustii/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./trustii/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./trustii/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: future in ./trustii/lib/python3.10/site-packages (from hyperopt) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle in ./trustii/lib/python3.10/site-packages (from hyperopt) (3.0.0)\n",
      "Requirement already satisfied: py4j in ./trustii/lib/python3.10/site-packages (from hyperopt) (0.10.9.7)\n",
      "Requirement already satisfied: six in ./trustii/lib/python3.10/site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.2 in ./trustii/lib/python3.10/site-packages (from hyperopt) (3.2.1)\n",
      "Requirement already satisfied: Mako in ./trustii/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4 in ./trustii/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./trustii/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./trustii/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn numpy xgboost optuna gplearn mlxtend seaborn matplotlib hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Additional data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used an additional dataset from this github repo: **https://github.com/zie225/Machine_learning-final_project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries and data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "train_data1 = pd.read_csv('train.csv')\n",
    "train_data2 = pd.read_csv('export_alimconfiance.csv', delimiter=\";\")\n",
    "test_data = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a new column in the new used data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adresse_2_UA is misssing in the new dataset so we will just replace missing values with _  ('_' also represent missing values in the datasets given by the compeition host)\n",
    "train_data2['Adresse_2_UA'] = '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52438, 13)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.concat([train_data1, train_data2], ignore_index=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(input_df):\n",
    "    # Convert the \"Date_inspection\" column to datetime format\n",
    "    input_df[\"Date_inspection\"] = pd.to_datetime(input_df[\"Date_inspection\"], utc=True)\n",
    "\n",
    "    # Extract day, year, and month\n",
    "    input_df[\"Inspection_Year\"] = input_df[\"Date_inspection\"].dt.year\n",
    "    input_df[\"Inspection_Month\"] = input_df[\"Date_inspection\"].dt.month\n",
    "    input_df[\"Inspection_Day\"] = input_df[\"Date_inspection\"].dt.day\n",
    "\n",
    "    # Extract Day of the Week\n",
    "    input_df[\"Inspection_DayOfWeek\"] = input_df[\"Date_inspection\"].dt.dayofweek\n",
    "\n",
    "    # Extract Week of the Year\n",
    "    input_df[\"Inspection_WeekOfYear\"] = input_df[\"Date_inspection\"].dt.isocalendar().week\n",
    "\n",
    "    # Extract Quarter\n",
    "    input_df[\"Inspection_Quarter\"] = input_df[\"Date_inspection\"].dt.quarter\n",
    "\n",
    "    # Extract Season\n",
    "    input_df[\"Inspection_Season\"] = (input_df[\"Date_inspection\"].dt.month % 12 + 3) // 3\n",
    "\n",
    "    # Text-based Features\n",
    "    # Fill missing values with a constant value\n",
    "    input_df['APP_Libelle_etablissement'] = input_df['APP_Libelle_etablissement'].fillna(\"Unknown\")\n",
    "    tfidf = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "    establishment_name_tfidf = tfidf.fit_transform(input_df['APP_Libelle_etablissement'].astype(str)).toarray()\n",
    "    tfidf_df = pd.DataFrame(establishment_name_tfidf, columns=[f'establishment_name_tfidf_{i}' for i in range(50)])\n",
    "    input_df = pd.concat([input_df, tfidf_df], axis=1)\n",
    "\n",
    "    # Length of Establishment Name and Address\n",
    "    input_df['address_length'] = input_df['Adresse_2_UA'].astype(str).apply(len)\n",
    "\n",
    "    # Temporal Features\n",
    "    input_df['is_weekend'] = (input_df['Inspection_DayOfWeek'] >= 5).astype(int)\n",
    "    input_df['is_business_hours'] = ((input_df['Date_inspection'].dt.hour >= 9) & (input_df['Date_inspection'].dt.hour <= 17)).astype(int)\n",
    "\n",
    "    # Drop the original \"Date_inspection\" column\n",
    "    input_df.drop(\"Date_inspection\", axis=1, inplace=True)\n",
    "\n",
    "    # Fill missing values in 'Adresse_2_UA' based on the same 'Code_postal'\n",
    "    input_df['Adresse_2_UA'] = input_df.groupby('Code_postal')['Adresse_2_UA'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0]) if not x.isnull().all() else x\n",
    "    )\n",
    "\n",
    "    # Split 'geores' column into two columns\n",
    "    input_df['geores'] = input_df['geores'].fillna(\"0_0\")\n",
    "    input_df[['establishment_Longitude', 'establishment_Latitude']] = input_df['geores'].str.split('_', expand=True)\n",
    "\n",
    "    # Convert the new columns to float, keeping missing values\n",
    "    input_df[['establishment_Longitude', 'establishment_Latitude']] = input_df[['establishment_Longitude', 'establishment_Latitude']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Fill missing values in 'Adresse_2_UA' based on the same 'Code_postal'\n",
    "    input_df['Adresse_2_UA'] = input_df.groupby('Code_postal')['Adresse_2_UA'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0]) if not x.isnull().all() else x\n",
    "    )\n",
    "\n",
    "    # Fill missing values with a constant value\n",
    "    input_df['filtre'] = input_df['filtre'].fillna(\"Unknown\")\n",
    "\n",
    "    # Fill missing values in 'Agrement' with a constant value\n",
    "    input_df['Agrement'] = input_df['Agrement'].fillna(\"Unknown\")\n",
    "\n",
    "    # Fill missing values in 'establishment_Latitude' with the most common value\n",
    "    input_df['establishment_Latitude'] = input_df['establishment_Latitude'].fillna(input_df['establishment_Latitude'].mean())\n",
    "\n",
    "    # Fill missing values with a constant value in 'Adresse_2_UA'\n",
    "    input_df['Adresse_2_UA'] = input_df['Adresse_2_UA'].fillna(input_df['Adresse_2_UA'].mode().iloc[0])\n",
    "\n",
    "    # Additional features\n",
    "    input_df['establishment_name_length'] = input_df['APP_Libelle_etablissement'].apply(len)\n",
    "    input_df['activity_details_length'] = input_df['filtre'].apply(len)\n",
    "    input_df['industry_code'] = input_df['SIRET'].astype(str).str[:2]\n",
    "    input_df['region_code'] = input_df['SIRET'].astype(str).str[2:5]\n",
    "    input_df['street_name'] = input_df['Adresse_2_UA'].str.split(',').str[0]\n",
    "\n",
    "    # Convert 'industry_code' and 'region_code' to numeric with handling of errors\n",
    "    input_df['industry_code'] = pd.to_numeric(input_df['industry_code'], errors='coerce')\n",
    "    input_df['region_code'] = pd.to_numeric(input_df['region_code'], errors='coerce')\n",
    "\n",
    "    # Interaction Features\n",
    "    input_df['industry_region_interaction'] = input_df['industry_code'].astype(str) + '_' + input_df['region_code'].astype(str)\n",
    "\n",
    "    # Drop the original 'geores' column\n",
    "    input_df = input_df.drop('geores', axis=1)\n",
    "\n",
    "    # Fill missing values in 'industry_code' and 'establishment_Longitude' with the mean and median\n",
    "    input_df['industry_code'] = input_df['industry_code'].fillna(input_df['industry_code'].median())\n",
    "    input_df['establishment_Longitude'] = input_df['establishment_Longitude'].fillna(input_df['establishment_Longitude'].mean())\n",
    "    input_df['region_code'] = input_df['region_code'].fillna(input_df['region_code'].median())\n",
    "\n",
    "    return input_df\n",
    "\n",
    "# Assuming train_data and test_data are your original datasets\n",
    "train_data = preprocess_data(train_data)\n",
    "test_data = preprocess_data(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numerical features to floats\n",
    "numerical_features = ['Inspection_Year', 'Inspection_Month', 'Inspection_Day',\n",
    "    'Inspection_DayOfWeek', 'Inspection_WeekOfYear', 'Inspection_Quarter',\n",
    "    'Inspection_Season', 'establishment_Longitude',\n",
    "    'establishment_Latitude', 'establishment_name_length', 'activity_details_length',\n",
    "    'industry_code', 'region_code'\n",
    "]\n",
    "\n",
    "train_data[numerical_features] = train_data[numerical_features].astype(float)\n",
    "test_data[numerical_features] = test_data[numerical_features].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheking mssing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APP_Libelle_etablissement      0\n",
       "SIRET                          0\n",
       "Adresse_2_UA                   0\n",
       "Code_postal                    0\n",
       "Libelle_commune                0\n",
       "                              ..\n",
       "activity_details_length        0\n",
       "industry_code                  0\n",
       "region_code                    0\n",
       "street_name                    0\n",
       "industry_region_interaction    0\n",
       "Length: 79, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 59\u001b[0m\n\u001b[1;32m     50\u001b[0m best_xgb_classifier \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[1;32m     51\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti:softmax\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     52\u001b[0m     num_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(label_encoder\u001b[38;5;241m.\u001b[39mclasses_),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_hyperparameters\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Fit the model on the training set\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mbest_xgb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[1;32m     62\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_xgb_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/Desktop/trustii/vscode/trustii/lib/python3.10/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/trustii/vscode/trustii/lib/python3.10/site-packages/xgboost/sklearn.py:1519\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1491\u001b[0m (\n\u001b[1;32m   1492\u001b[0m     model,\n\u001b[1;32m   1493\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1499\u001b[0m )\n\u001b[1;32m   1500\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1501\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1502\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1517\u001b[0m )\n\u001b[0;32m-> 1519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/trustii/vscode/trustii/lib/python3.10/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/trustii/vscode/trustii/lib/python3.10/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/trustii/vscode/trustii/lib/python3.10/site-packages/xgboost/core.py:2051\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m     _check_call(\n\u001b[0;32m-> 2051\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2054\u001b[0m     )\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2056\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define categorical and numerical features that will be used \n",
    "categorical_features = ['APP_Libelle_etablissement', 'SIRET', 'Code_postal',\n",
    "                         'Libelle_commune', 'Numero_inspection', 'APP_Libelle_activite_etablissement',\n",
    "                         'filtre', 'ods_type_activite', 'street_name',\n",
    "                         'industry_region_interaction', 'Agrement']\n",
    "numerical_features = ['Inspection_Year', 'Inspection_Month', 'Inspection_Day',\n",
    "                       'Inspection_DayOfWeek', 'Inspection_WeekOfYear', 'Inspection_Quarter',\n",
    "                       'Inspection_Season', 'establishment_Longitude',\n",
    "                       'establishment_Latitude', 'establishment_name_length', 'activity_details_length',\n",
    "                       'industry_code', 'region_code']\n",
    "\n",
    "# Combine training and test data for ordinal encoding\n",
    "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Create an instance of OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# Fit and transform the categorical features on combined data\n",
    "encoded_categorical_features_combined = encoder.fit_transform(combined_data[categorical_features])\n",
    "\n",
    "# Display the transformed features for training data\n",
    "train_data_encoded = pd.DataFrame(encoded_categorical_features_combined[:len(train_data)], columns=categorical_features)\n",
    "\n",
    "# Combine numerical and encoded categorical features for the training data\n",
    "features_train = pd.concat([train_data_encoded, train_data[numerical_features]], axis=1)\n",
    "\n",
    "# Encode the target variable for training data\n",
    "label_encoder = LabelEncoder()\n",
    "target_encoded = label_encoder.fit_transform(train_data['Synthese_eval_sanit'])\n",
    "\n",
    "# Split the data into training and testing sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_train, target_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Use the best hyperparameters obtained from Optuna\n",
    "best_hyperparameters = {'subsample': 0.8807451487381295, 'n_estimators': 881, 'min_child_weight': 2,\n",
    "                        'max_depth': 14, 'learning_rate': 0.023535353535353534, 'colsample_bytree': 0.21950136279318744}\n",
    "\n",
    "# Initialize XGBoost classifier with the best hyperparameters\n",
    "best_xgb_classifier = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=len(label_encoder.classes_),\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    **best_hyperparameters\n",
    ")\n",
    "\n",
    "# Fit the model on the training set\n",
    "best_xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_xgb_classifier.predict(X_test)\n",
    "\n",
    "# Convert the predicted labels back to original class labels\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate and print accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Fit the model on the entire training set with the best hyperparameters\n",
    "best_xgb_classifier.fit(features_train, target_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features for test data\n",
    "encoded_categorical_features_test = encoder.transform(test_data[categorical_features])\n",
    "test_data_encoded = pd.DataFrame(encoded_categorical_features_test, columns=categorical_features)\n",
    "\n",
    "# Combine numerical and encoded categorical features for the test data\n",
    "test_features = pd.concat([test_data_encoded, test_data[numerical_features]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the test set using XGBoost classifier with top N features\n",
    "predictions = best_xgb_classifier.predict(test_features)\n",
    "predictions = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "\n",
    "test_data=pd.read_csv('test.csv')\n",
    "\n",
    "# Update the 'Synthese_eval_sanit' column with the XGBoost predictions using top N features\n",
    "test_data['Synthese_eval_sanit'] = predictions\n",
    "\n",
    "# Save the updated test_data to a CSV file\n",
    "test_data.to_csv('result.csv', index=False, encoding='UTF-8')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4454194,
     "sourceId": 7642280,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Pytorch 2.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

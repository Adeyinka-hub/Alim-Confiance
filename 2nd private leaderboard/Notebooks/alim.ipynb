{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f708554",
   "metadata": {},
   "source": [
    "**Utilizing External Data in Our Pre-processing Pipeline**\n",
    "\n",
    "Based on the recent discussion in the [Trustii Team forum](https://app.trustii.io/datasets/1519/forums/125/messages), it's been confirmed that we have the flexibility to incorporate any dataset into our pre-processing pipeline, with the caveat that this dataset should not contain any labels from our test set.\n",
    "\n",
    "In line with this, I've acquired an additional dataset titled **\"Sirene: Fichier StockEtablissement du 26 Mars 2024\"**. This comprehensive dataset can be manually downloaded from [Data.gouv.fr](https://www.data.gouv.fr/fr/datasets/base-sirene-des-entreprises-et-de-leurs-etablissements-siren-siret/#/resources), where you can also find detailed information about the file contents and column descriptions. Alternatively, you can directly initiate the download [here](https://www.data.gouv.fr/fr/datasets/r/0651fb76-bcf3-4f6a-a38d-bc04fa708576).\n",
    "\n",
    "This extensive dataset encompasses a wide array of entities in France, including essential information like Siret and Siren numbers, among other data. We've subsequently merged this with our dataset for enhanced analysis.\n",
    "\n",
    "**Please Note:** The size of the CSV file is substantial, and it cannot be read in its entirety at once. Below, I've provided a script that reads the data in chunks and extracts only the rows matching the Siret numbers in our dataset.\n",
    "\n",
    "### Comprehensive Guide to Utilize the Dataset:\n",
    "\n",
    "1. **Download the Dataset:**\n",
    "   Download the zip file \"Sirene: Fichier StockEtablissement du 26 Mars 2024\" from the [provided link](https://www.data.gouv.fr/fr/datasets/r/0651fb76-bcf3-4f6a-a38d-bc04fa708576).\n",
    "\n",
    "2. **Unzip the File:**\n",
    "   After downloading, extract the contents of the zip file.\n",
    "\n",
    "3. **Set Up the File Path:**\n",
    "   In the script below, assign `file_path` to the location of the unzipped file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bacc7e",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b700e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le =LabelEncoder()\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f0500",
   "metadata": {},
   "source": [
    "# Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e22eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = test[['trustii_id']]\n",
    "test.drop(['trustii_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "361b7ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['train'] = 1\n",
    "test['train'] = 0\n",
    "train_test = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbd3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['SIRET'] = train_test['SIRET'].apply(lambda x: float(x) if x.isnumeric() else 0)\n",
    "siret_list = train_test['SIRET'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29230af2",
   "metadata": {},
   "source": [
    "# SIRET Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dc62908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = r'Path/to/uncompressed/file'\n",
    "\n",
    "# Initialize an empty DataFrame to store the filtered rows\n",
    "filtered_df = pd.DataFrame()\n",
    "\n",
    "# Process the file in chunks\n",
    "chunksize = 100000  # You can adjust the chunk size based on your memory capacity\n",
    "for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize)):\n",
    "    filtered_chunk = chunk[chunk['siret'].isin(siret_list)]\n",
    "    filtered_df = pd.concat([filtered_df, filtered_chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cc537f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of NaN values in each column\n",
    "percent_nan = filtered_df.isnull().mean() * 100\n",
    "\n",
    "# Identify columns where the percentage of NaN values is more than 50%\n",
    "columns_to_drop_nan = percent_nan[percent_nan > 50].index\n",
    "\n",
    "# Identify columns with only one unique value\n",
    "columns_to_drop_unique = filtered_df.columns[filtered_df.nunique() <= 1]\n",
    "\n",
    "# Combine the two sets of columns to drop\n",
    "columns_to_drop = columns_to_drop_nan.union(columns_to_drop_unique)\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "df = filtered_df.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "032583dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the columns we are left with are  ['siren', 'nic', 'siret', 'statutDiffusionEtablissement', 'dateCreationEtablissement', 'trancheEffectifsEtablissement', 'etablissementSiege', 'nombrePeriodesEtablissement', 'numeroVoieEtablissement', 'typeVoieEtablissement', 'libelleVoieEtablissement', 'codePostalEtablissement', 'libelleCommuneEtablissement', 'codeCommuneEtablissement', 'dateDebut', 'etatAdministratifEtablissement', 'activitePrincipaleEtablissement', 'nomenclatureActivitePrincipaleEtablissement', 'caractereEmployeurEtablissement']\n"
     ]
    }
   ],
   "source": [
    "print(\"the columns we are left with are \", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e91f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dateCreationEtablissement'] = pd.to_datetime(df['dateCreationEtablissement'])\n",
    "\n",
    "# Create new columns in the DataFrame\n",
    "df['day_variable'] = df['dateCreationEtablissement'].dt.day\n",
    "df['month_variable'] = df['dateCreationEtablissement'].dt.month\n",
    "df['year_variable'] = df['dateCreationEtablissement'].dt.year\n",
    "df = df.drop(['dateCreationEtablissement'], axis=1)\n",
    "\n",
    "# Splitting the 'activitePrincipaleEtablissement' into multiple columns\n",
    "df['section'] = df['activitePrincipaleEtablissement'].str[0]\n",
    "df['division'] = df['activitePrincipaleEtablissement'].str[1:3]\n",
    "df['group'] = df['activitePrincipaleEtablissement'].str[3]\n",
    "df['class'] = df['activitePrincipaleEtablissement'].str[4]\n",
    "df['subclass'] = df['activitePrincipaleEtablissement'].str[4:]\n",
    "\n",
    "df['trancheEffectifsEtablissement'] = df['trancheEffectifsEtablissement'].fillna(-1).apply(lambda x:  float(x) if x!='NN' else 0)\n",
    "df['numeroVoieEtablissement'].fillna(-1).apply(lambda x:  float(x) if x!='[ND]' else 0)\n",
    "df = df.drop(['libelleCommuneEtablissement', 'codePostalEtablissement'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "916d2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.merge(train_test, df, left_on='SIRET', right_on='siret', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bcadea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APP_Libelle_etablissement                      0.000031\n",
       "SIRET                                          0.000000\n",
       "Adresse_2_UA                                   0.014295\n",
       "Code_postal                                    0.000000\n",
       "Libelle_commune                                0.000000\n",
       "Numero_inspection                              0.000000\n",
       "Date_inspection                                0.000000\n",
       "APP_Libelle_activite_etablissement             0.000000\n",
       "Synthese_eval_sanit                            0.300019\n",
       "Agrement                                       0.743732\n",
       "geores                                         0.026677\n",
       "filtre                                         0.257441\n",
       "ods_type_activite                              0.000000\n",
       "train                                          0.000000\n",
       "siren                                          0.003427\n",
       "nic                                            0.003427\n",
       "siret                                          0.003427\n",
       "statutDiffusionEtablissement                   0.003427\n",
       "trancheEffectifsEtablissement                  0.003427\n",
       "etablissementSiege                             0.003427\n",
       "nombrePeriodesEtablissement                    0.003427\n",
       "numeroVoieEtablissement                        0.297332\n",
       "typeVoieEtablissement                          0.184883\n",
       "libelleVoieEtablissement                       0.020254\n",
       "codeCommuneEtablissement                       0.003736\n",
       "dateDebut                                      0.003427\n",
       "etatAdministratifEtablissement                 0.003427\n",
       "activitePrincipaleEtablissement                0.003427\n",
       "nomenclatureActivitePrincipaleEtablissement    0.003427\n",
       "caractereEmployeurEtablissement                0.003427\n",
       "day_variable                                   0.003427\n",
       "month_variable                                 0.003427\n",
       "year_variable                                  0.003427\n",
       "section                                        0.003427\n",
       "division                                       0.003427\n",
       "group                                          0.003427\n",
       "class                                          0.003427\n",
       "subclass                                       0.003427\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c4e58",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e955897",
   "metadata": {},
   "source": [
    "Create new features:\n",
    "\n",
    "- `paris`: Flag indicating if the establishment is in Paris.\n",
    "- `type_adresse`: Categorize address type based on keywords.\n",
    "- `Agrement`: Flag indicating missing 'Agrement' value.\n",
    "- `filtre`: Flag indicating missing 'filtre' value.\n",
    "- `APP_Libelle_etablissement_count`: Count occurrences of 'APP_Libelle_etablissement'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c340703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['paris'] = train_test.Libelle_commune.apply(lambda x: int('paris' in x.lower()) )\n",
    "train_test['type_adresse'] = train_test[\"Adresse_2_UA\"].astype(str).apply(lambda x: 'rue' if 'rue' in x.lower() else ('av' if 'av' in x.lower() else ('ecole' if 'ecole' in x.lower() else 'autre' )))\n",
    "train_test['Agrement'] = train_test['Agrement'].apply(lambda x: 1 if pd.isna(x) else 0)\n",
    "\n",
    "# \"APP_Libelle_activite_etablissement\" and  'filtre' are the same\n",
    "train_test['filtre'] = train_test['filtre'].apply(lambda x: 1 if pd.isna(x) else 0)\n",
    "\n",
    "count = train_test['APP_Libelle_etablissement'].value_counts()\n",
    "train_test['APP_Libelle_etablissement_count'] = train_test['APP_Libelle_etablissement'].map(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbffdcd",
   "metadata": {},
   "source": [
    "# Activities \n",
    "\n",
    "The following code first processes the \"APP_Libelle_activite_etablissement\" column by standardizing the activity descriptions and splitting them into individual activities. Then, it creates new columns for various food-related categories and assigns a value of 1 to each category if the establishment's activities include any activity from that category's predefined list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb6b5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize and split activities\n",
    "def process_activities(activity):\n",
    "    if pd.isna(activity) or activity == '_':\n",
    "        return []\n",
    "    # Standardize: lower case and replace separators\n",
    "    standardized = activity.lower().replace('-', '|').replace('/', '|')\n",
    "    # Split into list of activities\n",
    "    return set(standardized.split('|'))\n",
    "\n",
    "# Apply the function to the column\n",
    "train_test['activities'] = train_test['APP_Libelle_activite_etablissement'].apply(process_activities)\n",
    "\n",
    "activity_categories = {\n",
    "    \"Meat and Poultry Processing\": [\n",
    "        'boucherie',\n",
    "        \"chaîne d'abattage d'animaux boucherie\",\n",
    "        \"chaîne d'abattage de volaille\",\n",
    "        'charcuterie',\n",
    "        'découpe de viande de boucherie',\n",
    "        'découpe de viandes de volailles',\n",
    "        'entreposage de viande de boucherie',\n",
    "        'entreposage de viande de volailles',\n",
    "        'rayon boucherie',\n",
    "        'vente en gros de viande de boucherie',\n",
    "        'vh_ vsm_ préparation viandes boucherie',\n",
    "        'vh_ vsm_ préparation viandes volailles lagomorphes',\n",
    "        \"production d'abats\",\n",
    "        'traitement du gibier sauvage',\n",
    "        'transformation de produits carnés',\n",
    "        'gibier ongulé élevage',\n",
    "        'lagomorphe',\n",
    "        'petit gibier',\n",
    "        'découpe de gibier sauvage',\n",
    "        \"découpe de gros gibier d'élevage\",\n",
    "    ],\n",
    "    \"Dairy and Egg Products\": [\n",
    "        'collecte de lait',\n",
    "        \"centre d'emballage des oeufs\",\n",
    "        \"collecteur d'oeufs\",\n",
    "        'découpe de fromages',\n",
    "        'entreposage de produits laitiers',\n",
    "        'rayon fromagerie',\n",
    "        \"production d'œuf liquide d'ovoproduit et produits à base œuf\",\n",
    "        'transformation de lait ou produits laitiers',\n",
    "        'fromagerie',\n",
    "        'déshydratation de lait ou produits laitiers',\n",
    "        'elevage de bovins',\n",
    "    ],\n",
    "    \"Fish and Seafood Processing\": [\n",
    "        \"abattage de produits de l'aquaculture\",\n",
    "        'entreposage de produits de la pêche',\n",
    "        'expédition de coquillages',\n",
    "        'mareyage et préparation de produits de la pêche',\n",
    "        'poissonnerie',\n",
    "        'rayon poissonnerie',\n",
    "        'vente en gros produits de la pêche',\n",
    "        \"plateforme d'éclatement de produits de la pêche\",\n",
    "        'transformation de produits de la pêche',\n",
    "        'vivier produits de la pêche',\n",
    "    ],\n",
    "    \"Bakery and Confectionery\": [\n",
    "        'boulangerie',\n",
    "        'pâtisserie',\n",
    "        'chocolatier',\n",
    "        'viennoiserie',\n",
    "        'glacier',\n",
    "    ],\n",
    "    \"General Food Retail and Services\": [\n",
    "        'alimentation générale',\n",
    "        'commerce alimentaire',\n",
    "        'distribution automatique',\n",
    "        'libre service',\n",
    "        'producteur fermier',\n",
    "        'restaurant',\n",
    "        'restauration collective',\n",
    "        'responsable de restauration collective',\n",
    "        'rayon traiteur',\n",
    "        'traiteur',\n",
    "    ],\n",
    "    \"Specialized Food Processing and Handling\": [\n",
    "        \"collecte de gibier sauvage\",\n",
    "        \"production de produits à base d'escargots\",\n",
    "        'production de produits à base de grenouilles',\n",
    "        'préparation de produits composés',\n",
    "        'purification',\n",
    "        \"caves d'affinage\",\n",
    "        'halle à m.',\n",
    "        'infrastructure de marché',\n",
    "        'entreposage non spécialisé de denrées alimentaires',\n",
    "        'entreposage de distribution de denrées alimentaires',\n",
    "        'transport de denrées alimentaires',\n",
    "        'pêche de production primaire',\n",
    "        'gestion administrative',\n",
    "        \"salle d'abattage à la farme\",\n",
    "        \"production de collagène, de gélatine ou de phr\",\n",
    "        'autres activités de remise directe',\n",
    "        'primeur',\n",
    "        'métier de bouche',\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, activities in activity_categories.items():\n",
    "    train_test[category] = train_test['activities'].apply(lambda x: 1 if any(activity in x for activity in activities) else 0)\n",
    "train_test['activities'] = train_test['activities'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de0bf4",
   "metadata": {},
   "source": [
    "## Numero Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08bfae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['numero_inspection_1'] = train_test['Numero_inspection'].apply(lambda x: x[:2]).astype(float)\n",
    "train_test['numero_inspection_2'] = train_test['Numero_inspection'].apply(lambda x: int(x[3:-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839c2d4",
   "metadata": {},
   "source": [
    "## Postal code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95cf06fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['region_postal_code'] = train_test['Code_postal'].apply(lambda x: x[:2] if x[:2]!='AD' and x[:2]!='0[' else 100 ).astype(float)\n",
    "train_test['sub_region_postal_code'] = train_test['Code_postal'].apply(lambda x: x[2:] if x[2:]!= '0NR' and x[2:]!='ND]' else 0).astype(float)\n",
    "\n",
    "count = train_test['region_postal_code'].value_counts()\n",
    "train_test['region_postal_code_count'] = train_test['region_postal_code'].map(count)\n",
    "train_test['Code_postal'] = pd.to_numeric(train_test['Code_postal'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde6484c",
   "metadata": {},
   "source": [
    "## Siret count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00cdd62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = train_test['SIRET'].value_counts()\n",
    "train_test['SIRET_count'] = train_test['SIRET'].map(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe94fe2",
   "metadata": {},
   "source": [
    "## Date Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e506d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date_inspection' is in datetime format\n",
    "train_test['Date_inspection'] = pd.to_datetime(train_test['Date_inspection'])\n",
    "\n",
    "# Extract year, month, and day directly from datetime.datetime objects\n",
    "train_test['month'] = train_test['Date_inspection'].apply(lambda x: x.month)\n",
    "train_test['day'] = train_test['Date_inspection'].apply(lambda x: x.day)\n",
    "\n",
    "first_inspection_date = train_test['Date_inspection'].min()\n",
    "\n",
    "train_test['day_of_week'] = train_test['Date_inspection'].apply(lambda x: x.weekday())\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "\n",
    "train_test['season'] = train_test['Date_inspection'].apply(lambda x: get_season(x.month))\n",
    "\n",
    "train_test.drop(['Date_inspection'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4597b06",
   "metadata": {},
   "source": [
    "## Geolocation Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6e8ba",
   "metadata": {},
   "source": [
    "- `Extract_geo`: Extract latitude and longitude from 'geores'.\n",
    "- `Impute_geo`: Impute missing values with mean.\n",
    "- `Distance_fixed_point`: Calculate distance to a fixed point in France.\n",
    "- `Cluster_geo`: Cluster establishments based on latitude and longitude.\n",
    "- `Region_center_coordinates`: Calculate region center coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a207a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test[['lat', 'lon']] = train_test['geores'].str.split('_', expand=True)\n",
    "\n",
    "# Convert 'lat' and 'lon' from string to float\n",
    "train_test['lat'] = train_test['lat'].astype(float)\n",
    "train_test['lon'] = train_test['lon'].astype(float)\n",
    "\n",
    "train_test['lat'] = train_test['lat'].fillna(train_test['lat'].mean())\n",
    "train_test['lon'] = train_test['lon'].fillna(train_test['lon'].mean())\n",
    "train_test.drop(['geores'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2c5fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! maybe consider the distance from the centre of each city\n",
    "from math import cos, sin, radians, sqrt\n",
    "\n",
    "# 2. Haversine Distance\n",
    "def haversine(lat_lon1, lat_lon2):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1 = map(np.radians, lat_lon1)\n",
    "    lat2, lon2 = map(np.radians, lat_lon2)\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "fixed_point = (46.824, 2.298)  # roughly the centre of France (latitude, longitude)\n",
    "train_test['distance_to_fixed_point'] = train_test.apply(lambda row: haversine((row['lat'], row['lon']), fixed_point), axis=1)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(train_test[['lat', 'lon']])\n",
    "\n",
    "# Getting the cluster assignments\n",
    "labels = kmeans.labels_\n",
    "\n",
    "train_test['cluster'] = labels\n",
    "\n",
    "lat_center = train_test.groupby('region_postal_code').lat.mean()\n",
    "lon_center = train_test.groupby('region_postal_code').lon.mean()\n",
    "train_test['region_lat_center'] = train_test['region_postal_code'].map(lat_center)\n",
    "train_test['region_lon_center'] = train_test['region_postal_code'].map(lon_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c490b",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "308a58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Loop through each object type column and apply LabelEncoder\n",
    "for col in train_test.columns:\n",
    "    if train_test[col].dtype == 'object' and col!= 'Synthese_eval_sanit':\n",
    "        train_test[col] = le.fit_transform(train_test[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d85e9",
   "metadata": {},
   "source": [
    "# Target Encoding\n",
    "Encode 'Synthese_eval_sanit' with appropriate mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15156150",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.Synthese_eval_sanit.unique()\n",
    "encoding = {'Satisfaisant':2, 'Très satisfaisant':3, 'A améliorer':1, 'A corriger de manière urgente':0, np.nan:np.nan}\n",
    "train_test['Synthese_eval_sanit'] = train_test['Synthese_eval_sanit'].map(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63b044",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07964ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train_test.fillna(train_test.mean())\n",
    "train = train_test[train_test.train == 1].drop(['train'], axis=1)\n",
    "\n",
    "test = train_test[train_test.train == 0].drop(['train', 'Synthese_eval_sanit'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d3f1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop([\"Synthese_eval_sanit\"], axis=1)\n",
    "y = train.Synthese_eval_sanit\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e705513",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10f72077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(pred_probs, dmatrix):\n",
    "    labels = dmatrix.get_label()\n",
    "    acc = accuracy_score(labels, pred_probs)\n",
    "    return '1-merror', acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce68175f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-1-merror:0.67385\tvalidation_1-1-merror:0.63109\n",
      "[100]\tvalidation_0-1-merror:0.73329\tvalidation_1-1-merror:0.67519\n",
      "[200]\tvalidation_0-1-merror:0.76334\tvalidation_1-1-merror:0.68644\n",
      "[300]\tvalidation_0-1-merror:0.79422\tvalidation_1-1-merror:0.69217\n",
      "[400]\tvalidation_0-1-merror:0.82631\tvalidation_1-1-merror:0.69349\n",
      "[500]\tvalidation_0-1-merror:0.85289\tvalidation_1-1-merror:0.69746\n",
      "[600]\tvalidation_0-1-merror:0.87654\tvalidation_1-1-merror:0.69857\n",
      "[700]\tvalidation_0-1-merror:0.89694\tvalidation_1-1-merror:0.70055\n",
      "[800]\tvalidation_0-1-merror:0.91597\tvalidation_1-1-merror:0.70143\n",
      "[900]\tvalidation_0-1-merror:0.93190\tvalidation_1-1-merror:0.70320\n",
      "[1000]\tvalidation_0-1-merror:0.94492\tvalidation_1-1-merror:0.70187\n",
      "[1100]\tvalidation_0-1-merror:0.95694\tvalidation_1-1-merror:0.70165\n",
      "[1200]\tvalidation_0-1-merror:0.96482\tvalidation_1-1-merror:0.70408\n",
      "[1300]\tvalidation_0-1-merror:0.97326\tvalidation_1-1-merror:0.70364\n",
      "[1400]\tvalidation_0-1-merror:0.97905\tvalidation_1-1-merror:0.70540\n",
      "[1500]\tvalidation_0-1-merror:0.98346\tvalidation_1-1-merror:0.70342\n",
      "[1600]\tvalidation_0-1-merror:0.98677\tvalidation_1-1-merror:0.70342\n",
      "[1700]\tvalidation_0-1-merror:0.98996\tvalidation_1-1-merror:0.70408\n",
      "[1799]\tvalidation_0-1-merror:0.99189\tvalidation_1-1-merror:0.70342\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=0.7, colsample_bynode=0.7, colsample_bytree=0.9,\n",
       "              device=None, disable_default_eval_metric=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1800, n_jobs=None, num_class=4, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=0.7, colsample_bynode=0.7, colsample_bytree=0.9,\n",
       "              device=None, disable_default_eval_metric=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1800, n_jobs=None, num_class=4, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=0.7, colsample_bynode=0.7, colsample_bytree=0.9,\n",
       "              device=None, disable_default_eval_metric=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1800, n_jobs=None, num_class=4, ...)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "params = {\n",
    " 'n_jobs':-1,\n",
    " 'disable_default_eval_metric':1,\n",
    " 'num_class':4,\n",
    " 'objective':'multi:softmax',\n",
    " 'random_state':0,\n",
    " 'colsample_bylevel': 0.6,\n",
    " 'colsample_bynode': 0.7,\n",
    " 'colsample_bytree': 0.9,\n",
    " 'device': 'cpu',\n",
    " 'learning_rate': 0.015,\n",
    " 'max_depth': 8,\n",
    " 'n_estimators': 2000,\n",
    " 'num_parallel_tree': 2,\n",
    " 'random_state': 0,\n",
    " 'reg_lambda': 1.3,\n",
    " 'tree_method': 'gpu_hist'}\n",
    "\n",
    "clf = xgb.XGBClassifier(**params)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=accuracy, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51d9353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7034178610804851\n",
      "Confusion Matrix: \n",
      " [[   0    0   21    1]\n",
      " [   0   25  195   45]\n",
      " [   0    3 2230  369]\n",
      " [   0    3  708  935]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2bef509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=0.7, colsample_bynode=0.7, colsample_bytree=0.9,\n",
       "              device=None, disable_default_eval_metric=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1800, n_jobs=None, num_class=4, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=0.7, colsample_bynode=0.7, colsample_bytree=0.9,\n",
       "              device=None, disable_default_eval_metric=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1800, n_jobs=None, num_class=4, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=0.7, colsample_bynode=0.7, colsample_bytree=0.9,\n",
       "              device=None, disable_default_eval_metric=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.015, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1800, n_jobs=None, num_class=4, ...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train on the whole dataset\n",
    "clf = xgb.XGBClassifier(**params)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d107f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_encoding = {v:k for k,v in encoding.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c45d5",
   "metadata": {},
   "source": [
    "# Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1806ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Synthese_eval_sanit'] = clf.predict(test)\n",
    "submission['Synthese_eval_sanit'] = submission['Synthese_eval_sanit'].map(reverse_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6ec65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56756101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
